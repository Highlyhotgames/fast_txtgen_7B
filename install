#!/bin/bash
cd webui; echo -e "\033[32mActivating Miniconda Environment 'webui' ...\033[0m"; conda activate webui > /dev/null 2>&1; echo -e "\033[32mCloning Bits and Bytes from TimDettmers ...\033[0m"; git clone https://github.com/TimDettmers/bitsandbytes.git > /dev/null 2>&1; echo -e "\033[32mInstalling Ninja and CChardet ...\033[0m"; conda install ninja --yes > /dev/null 2>&1; pip install cchardet chardet > /dev/null 2>&1; echo -e "\033[1;34mChecking CUDA Installation ...\033[0m"; echo -e "\033[1;96m$(nvcc --version)\033[0m"; echo -e "\033[32mFixing Bitsandbytes to an older version ...\033[0m"; cd bitsandbytes; git reset --hard 49a04253fb1f3e195cb0e9e79bdb01db7a490774 > /dev/null 2>&1; echo -e "\033[32mInstalling pip requirements for Bitsandbytes ...\033[0m"; pip install -r requirements.txt > /dev/null 2>&1; echo -e "\033[32mMaking CUDA to 117 ...\033[0m"; CUDA_VERSION=117 make cuda11x > /dev/null 2>&1; echo -e "\033[32mInstalling setup.py from Bitsandbytes ...\033[0m"; python setup.py install > /dev/null 2>&1; cd ..; echo -e "\033[32mCloning Text Generation WebUI from Oobabooga ...\033[0m"; git clone https://github.com/oobabooga/text-generation-webui.git > /dev/null 2>&1; cd text-generation-webui; echo -e "\033[32mFixing Text Generation WebUI to an older version ...\033[0m"; git reset --hard fcda3f87767e642d1c0411776e549e1d3894843d > /dev/null 2>&1; echo -e "\033[32mCreating repositories folder ...\033[0m"; mkdir repositories; cd repositories; echo -e "\033[32mCloning Oobabooga's CUDA branch for GPTQ-for-LLaMa (from qwopqwop200) ...\033[0m"; git clone --branch cuda https://github.com/oobabooga/GPTQ-for-LLaMa.git > /dev/null 2>&1; cd GPTQ-for-LLaMa; echo -e "\033[32mFixing GPTQ-for-LLaMa to an older version ...\033[0m"; git reset --hard a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 > /dev/null 2>&1; echo -e "\033[32mInstalling pip requirements for GPTQ-for-LLaMa ...\033[0m"; rm requirements.txt; cp ~/fast_txtgen_7B/req_1 requirements.txt; pip install -r requirements.txt > /dev/null 2>&1; echo -e "\033[32mInstalling setup_cuda.py from GPTQ-for-LLaMa ...\033[0m"; python setup_cuda.py install > /dev/null 2>&1; cd ../..; echo -e "\033[32mInstalling pip requirements for Text Generation WebUI ...\033[0m"; rm requirements.txt; cp ~/fast_txtgen_7B/req_2 requirements.txt; pip install -r requirements.txt > /dev/null 2>&1; echo -e "\033[32mConverting magnet link to a .torrent file ...\033[0m"; aria2c --bt-metadata-only=true --bt-save-metadata=true "magnet:?xt=urn:btih:88f7d9d2460ffcaf78b21e83012de00939eacb65&dn=LLaMA-HF-4bit-128g&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce" > /dev/null 2>&1; mv 88f7d9d2460ffcaf78b21e83012de00939eacb65.torrent LLaMA-HF-4bit-128g.torrent; echo -e "\033[32mDownloading LLaMA 7B ...\033[0m"; aria2c LLaMA-HF-4bit-128g.torrent --select-file=2-7,25 -d ~/webui/text-generation-webui/models --auto-file-renaming=false --quiet=false --allow-overwrite=true --max-connection-per-server=4 --seed-time=0 --summary-interval=0 --disable-ipv6=true; cd models/LLaMA-HF-4bit-128g; mv llama-7b-4bit-128g ../; cd ..; rm -r LLaMA-HF-4bit-128g; cd ..;
python server.py --wbits 4 --model llama-7b-4bit-128g --groupsize 128 --chat
